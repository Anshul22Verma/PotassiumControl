{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_excel(\"/Users/vermaa/Documents/photon-ai-module/GDMT_data_stacked_KRYMAI.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows:  188\n",
      "Number of Rows after dropping patients without diet history:  47\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Rows: \", len(df))\n",
    "df = df.dropna(subset=[\"diet_history\"])\n",
    "print(\"Number of Rows after dropping patients without diet history: \", len(df))\n",
    "\n",
    "history = df[\"diet_history\"].values.tolist()\n",
    "recommendation = df[\"follow_up_recommendations\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_prompt = '''\n",
    "    You are a dietician espically dealing with patients that are on Heart-Failure, in particular \n",
    "    Heart failure patients with elevated potassium levels who are potential candidates for Mineralocorticoid Receptor Antagonist (MRA) treatment.\n",
    "\n",
    "    You will be provided with patient diet history in text with different food that they intake and timing of their meals.\n",
    "    You need to take in their history and provide them with recommendation to manage potassium level in their diet.\n",
    "    An example for this interation is:\n",
    "    Diet History: '9 am-gluten free bagel with peanut butter or salted butter or plain or frozen waffles/pancake-mix with real maple syrup or gluten-free oatmeal made with water plus brown sugar (1 TBSP) and berries 1300 hrs- cheese (2 slices of cheddar) or tuna sandwich plus dry apricots and 1 apple or gluten-free crackers with cheese or leftovers 5:50 pm-Fish with 2 cups rice with asparagus (6) and green beans (1 cup) or hamburger patty no bun and salad with prepared or homemade dressing or chicken breast (5 oz) with rice and vegs-1.5 cups (peppers, mushrooms, carrot, turnip, cabbage, tomatoes'\n",
    "    Recommendation: '1. Limit low potassium fruits to 3 servings/day 2. Limit low potassium vegs to 3 servings/day 3. Limit milk/yogurt/ice cream to 1 cup per day 4. Limit mushrooms to 1/2 cup 1 time per week 5. Limit tomato sauce to 1/4 cup per week 6. Limit tomatoes to 1/2 small tomato or 5 cherry tomatoes per day 7. Read the nutrition facts table to optimize choices'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_claude_3(AWS_ACCESS_KEY, AWS_SECRET_KEY, prompt_, ocr_prompt):\n",
    "    # Initialize the Amazon Bedrock runtime client    \n",
    "    client = boto3.client(service_name=\"bedrock-runtime\", region_name=\"us-east-1\", \n",
    "    aws_access_key_id= AWS_ACCESS_KEY, aws_secret_access_key= AWS_SECRET_KEY)\n",
    "\n",
    "    # model_id, fixed to using haiku for now\n",
    "    model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "    \n",
    "    content = [ {\"type\": \"text\",\n",
    "        \"text\": prompt_}, {\"type\": \"text\",\n",
    "        \"text\": ocr_prompt} ]\n",
    "    request_body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 2000,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content,\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    response = client.invoke_model(\n",
    "        modelId=model_id,\n",
    "        body=json.dumps(request_body),\n",
    "    )\n",
    "\n",
    "    # Process and print the response\n",
    "    result = json.loads(response.get(\"body\").read())\n",
    "    return result['content'][0]['text']       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY = \"****\"\n",
    "AWS_SECRET_KEY = \"****\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [06:30<00:00,  8.31s/it]\n"
     ]
    }
   ],
   "source": [
    "haiku_recommendations = []\n",
    "for his in tqdm(history, total=len(history)):\n",
    "    resp = invoke_claude_3(AWS_ACCESS_KEY, AWS_SECRET_KEY, prompt_=f'Diet history of patient is: {his}', ocr_prompt=ocr_prompt)\n",
    "    haiku_recommendations.append(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_openai(OPENAI_API_KEY, prompt_, ocr_prompt):\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    openai_turbo_model_name = \"gpt-4-turbo-preview\"\n",
    "    content = [{\"role\": \"system\",\n",
    "                    \"content\": ocr_prompt},\n",
    "                {\"role\": \"user\",\n",
    "                    \"content\": prompt_}]\n",
    "        \n",
    "    response = client.chat.completions.create(\n",
    "        model=openai_turbo_model_name,\n",
    "        stop=\"stop\",\n",
    "        top_p=0.0,\n",
    "        temperature=0.0,\n",
    "        messages=content,\n",
    "        max_tokens=2000\n",
    "    )\n",
    "    return str(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"****\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [14:42<00:00, 18.77s/it]\n"
     ]
    }
   ],
   "source": [
    "gpt4_recommendations = []\n",
    "for his in tqdm(history, total=len(history)):\n",
    "    resp = invoke_openai(OPENAI_API_KEY, prompt_=f'Diet history of patient is: {his}', ocr_prompt=ocr_prompt)\n",
    "    gpt4_recommendations.append(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame()\n",
    "res_df[\"diet_history\"] = history\n",
    "res_df[\"dietician_recommendation\"] = recommendation\n",
    "res_df[\"haiku_zero_shot_recommendation\"] = haiku_recommendations\n",
    "res_df[\"gpt4_zero_shot_recommendation\"] = gpt4_recommendations\n",
    "\n",
    "res_df.to_csv(\"result_recommendation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPARING THE RECOMMENDATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vermaa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/vermaa/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from textstat.textstat import textstat\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from transformers import pipeline\n",
    "\n",
    "# Ensure nltk is properly downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Precision, Recall, F1-Score (Token-level calculation)\n",
    "def precision_recall_f1(generated_text, reference_text):\n",
    "    generated_tokens = nltk.word_tokenize(generated_text.lower())\n",
    "    reference_tokens = nltk.word_tokenize(reference_text.lower())\n",
    "    \n",
    "    # True positives: Tokens in both reference and generated texts\n",
    "    true_positives = sum(1 for token in generated_tokens if token in reference_tokens)\n",
    "    \n",
    "    # False positives: Tokens in generated text but not in reference text\n",
    "    false_positives = sum(1 for token in generated_tokens if token not in reference_tokens)\n",
    "    \n",
    "    # False negatives: Tokens in reference text but not in generated text\n",
    "    false_negatives = sum(1 for token in reference_tokens if token not in generated_tokens)\n",
    "    \n",
    "    # Calculate Precision, Recall, F1\n",
    "    if true_positives + false_positives == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        \n",
    "    if true_positives + false_negatives == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "# 2. BLEU Score\n",
    "def bleu_score(reference_text, generated_text):\n",
    "    reference_tokens = nltk.word_tokenize(reference_text.lower())\n",
    "    generated_tokens = nltk.word_tokenize(generated_text.lower())\n",
    "\n",
    "    # Using smoothing function to avoid zero BLEU scores\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    return sentence_bleu([reference_tokens], generated_tokens, smoothing_function=smoothing_function)\n",
    "\n",
    "\n",
    "# 3. ROUGE Score\n",
    "def rouge_score_fn(reference_text, generated_text):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference_text, generated_text)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# 4. Perplexity Score (for Language Model)\n",
    "def perplexity(generated_text):\n",
    "    # Tokenize and compute perplexity using a simple unigram model\n",
    "    tokens = nltk.word_tokenize(generated_text.lower())\n",
    "    unique_tokens = set(tokens)\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    # Compute unigram probabilities\n",
    "    probabilities = [tokens.count(token) / token_count for token in unique_tokens]\n",
    "    entropy = -sum([p * math.log2(p) for p in probabilities])\n",
    "    return math.pow(2, entropy)\n",
    "\n",
    "\n",
    "# 5. Flesch-Kincaid Readability Score\n",
    "def flesch_kincaid_readability(generated_text):\n",
    "    return textstat.flesch_kincaid_grade(generated_text)\n",
    "\n",
    "\n",
    "# 6. Entropy\n",
    "def entropy(generated_text):\n",
    "    # Tokenize the text and compute entropy\n",
    "    tokens = nltk.word_tokenize(generated_text.lower())\n",
    "    token_count = len(tokens)\n",
    "    token_frequencies = {token: tokens.count(token) / token_count for token in set(tokens)}\n",
    "    entropy_value = -sum([p * math.log2(p) for p in token_frequencies.values()])\n",
    "    return entropy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f, bleu, rogue1, rogue2, rogueL, pp, fk, ent = [], [], [], [], [], [], [], [], [], []\n",
    "for reference_text, generated_text in zip(recommendation[1:], haiku_recommendations[1:]):\n",
    "    s = precision_recall_f1(generated_text, reference_text)\n",
    "    p.append(s[0])\n",
    "    r.append(s[1])\n",
    "    f.append(s[2])\n",
    "\n",
    "    bleu.append(bleu_score(reference_text, generated_text))\n",
    "    s = rouge_score_fn(reference_text, generated_text)\n",
    "    rogue1.append(s['rouge1'].fmeasure)\n",
    "    rogue2.append(s['rouge2'].fmeasure)\n",
    "    rogueL.append(s['rougeL'].fmeasure)\n",
    "\n",
    "    pp.append(perplexity(generated_text))\n",
    "    fk.append(flesch_kincaid_readability(generated_text))\n",
    "    ent.append(entropy(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Haiku recommendation performance:\n",
      "      \n",
      " Precision: 0.2413, Recall: 0.7595, F1-score: 0.3558\n",
      "      \n",
      " BLEU Score: 0.0087, ROGUE-1: 0.2004, ROGUE-2: 0.0556, ROGUE-L: 0.1191\n",
      "      \n",
      " Perplexity: 107.437, Flesch-Kincaid Readability Grade: 9.9413, Entropy: 6.7431\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Haiku recommendation performance:\n",
    "      \\n Precision: {round(sum(p)/len(p), 4)}, Recall: {round(sum(r)/len(r), 4)}, F1-score: {round(sum(f)/len(f), 4)}\n",
    "      \\n BLEU Score: {round(sum(bleu)/len(bleu), 4)}, ROGUE-1: {round(sum(rogue1)/len(rogue1), 4)}, ROGUE-2: {round(sum(rogue2)/len(rogue2), 4)}, ROGUE-L: {round(sum(rogueL)/len(rogueL), 4)}\n",
    "      \\n Perplexity: {round(sum(pp)/len(pp), 4)}, Flesch-Kincaid Readability Grade: {round(sum(fk)/len(fk), 4)}, Entropy: {round(sum(ent)/len(ent), 4)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f, bleu, rogue1, rogue2, rogueL, pp, fk, ent = [], [], [], [], [], [], [], [], [], []\n",
    "for reference_text, generated_text in zip(recommendation[1:], gpt4_recommendations[1:]):\n",
    "    s = precision_recall_f1(generated_text, reference_text)\n",
    "    p.append(s[0])\n",
    "    r.append(s[1])\n",
    "    f.append(s[2])\n",
    "\n",
    "    bleu.append(bleu_score(reference_text, generated_text))\n",
    "    s = rouge_score_fn(reference_text, generated_text)\n",
    "    rogue1.append(s['rouge1'].fmeasure)\n",
    "    rogue2.append(s['rouge2'].fmeasure)\n",
    "    rogueL.append(s['rougeL'].fmeasure)\n",
    "\n",
    "    pp.append(perplexity(generated_text))\n",
    "    fk.append(flesch_kincaid_readability(generated_text))\n",
    "    ent.append(entropy(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-4-turbo recommendation performance:\n",
      "      \n",
      " Precision: 0.2247, Recall: 0.8223, F1-score: 0.3444\n",
      "      \n",
      " BLEU Score: 0.0053, ROGUE-1: 0.1694, ROGUE-2: 0.0405, ROGUE-L: 0.0977\n",
      "      \n",
      " Perplexity: 117.8841, Flesch-Kincaid Readability Grade: 10.8239, Entropy: 6.8746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "GPT-4-turbo recommendation performance:\n",
    "      \\n Precision: {round(sum(p)/len(p), 4)}, Recall: {round(sum(r)/len(r), 4)}, F1-score: {round(sum(f)/len(f), 4)}\n",
    "      \\n BLEU Score: {round(sum(bleu)/len(bleu), 4)}, ROGUE-1: {round(sum(rogue1)/len(rogue1), 4)}, ROGUE-2: {round(sum(rogue2)/len(rogue2), 4)}, ROGUE-L: {round(sum(rogueL)/len(rogueL), 4)}\n",
    "      \\n Perplexity: {round(sum(pp)/len(pp), 4)}, Flesch-Kincaid Readability Grade: {round(sum(fk)/len(fk), 4)}, Entropy: {round(sum(ent)/len(ent), 4)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "physbench-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
